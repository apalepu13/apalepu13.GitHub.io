---
layout: main
---

<body>
  <div class="container">

      <div class="col-xs-12 col-sm-9 col-md-9">

        <div class="row"><p>
            <strong> Self-supervised learning for chest x-rays and reports</strong><ul>
            <li> Rationale: <ul>
                <li> Medical imaging, such as chest x-rays, are rich with information. However, they are not always labeled, and even when they are, these labels may only capture a fraction of the information a radiologist considers when evaluating the image. </li>
                <li> While labels are not always present or ideal, radiology reports that discuss clinically relevant information often accompany medical imaging. </li>
                <li> Self-supervised image-text architectures are an opportunity to leverage these radiology reports to learn from rich medical imaging and paired text data. </li>
                <li> We believe this type of architecture not only can yield better image representation, but also can allow us to: <ul> <li> accurately classify relevant findings in a zero-shot manner based on text descriptions </li> <li>identify for which inputs our model can be reliably used </li>  <li>reduce reliance on spurious correlations that traditional CNNs are subject to. </li></ul></li>
            </ul></li>
            <li> Project:<ul>
                <li> We trained a modified version of CLIP on MIMIC-CXR data and used CheXpert and Padchest datasets for evaluation. </li>
                <li> We generated synthetic watermark shortcuts to evaluate shortcut learning. </li>
                <li> We introduced new terms to the loss to impose sparsity of image-patch and text-token embeddings.</li>
            </ul>
            </li>
            <li>Outcomes:
                <ul class="nav nav-pills nav-stacked">
                <li> We demonstrated through synthetically-generated watermark shortcuts on chest x-rays that supervised CNNs are heavily reliant and unable to unlearn shortcuts, and self-supervision with text helps to reduce this reliance.</li>
                <li> We developed a regularized version of the model that achieves SOTA zero-shot classification AUCs, better than a comparable fully-supervised CNN on external test data.</li>
                <li> Using descriptive text prompting, we were able to classify novel diseases not present in training data, such as covid19. Our performance is competitive, and in some cases, exceeds that of radiologists.</li>
                </ul>
            </li>
        </ul> <br>
            <strong> Quantifying the trans-arterial embolization endpoint.</strong><ul>
            <li> Rationale: <ul>
                <li> Transarterial embolization (TAE) and its variants are effective minimally invasive procedures, often used to treat liver and kidney cancers, uterine fibroids, and other conditions. </li>
                <li> The procedure works by inserting a catheter into an artery supplying the tumor, and injecting beads to block its blood supply and essentially starve the tumor. Oftentimes, chemotherapy or radiological particles are injected alongside these embolic beads.</li>
                <li> While highly effective, it is important that the correct amount of beads are injected: inject too few, and the cancer won't be adequately treated; inject too many, and the beads (and chemotherapy!) can reach off-target tissue causing additional harm to an already sick patient. </li>
                <li> The current standard used by many clinicians to determine when to stop injecting is "beats of stasis". Using an angiogram, they inject contrast into the vasculature and count how long it takes for the contrast to wash away. </li>
                <li> However, this counting process is highly variable between clinicians, some opting not to use the method at all, and five seconds is a somewhat arbitrary cutoff.</li>
            </ul></li>
            <li> Project:<ul>
                <li> We aimed to quantify an embolization endpoint that could determine the optimal point to stop injecting, when blood flow was sufficiently blocked but risk of off-target embolization was acceptable. </li>
                <li> We designed a catheter device that occluded a vessel before embolization, so that we could pre-emptively measure the final target pressure clinicians should aim for when injecting beads downstream.</li>
                <li> We conducted in-vitro research to characterize the relationship between injection pressure, vessel pressure, and off-target embolization.</li>
            </ul>
            </li>
            <li>Outcomes:
                <ul class="nav nav-pills nav-stacked">
                <li> We were able to demonstrate that the pre-embolization predicted endpoint we provided aligned well with the final vessel pressure post-embolization.</li>
                <li> We used computer vision to identify instances of off-target embolization, and used this to create quantitative bounds on safe injection pressures that could be used at a given vessel pressure without causing off-target embolization.  </li>
                </ul>
            </li>
        </ul>
            <div class="img-parent-box" style="text-align:center">
                <div style="float:left;margin-right:5px;">
                <img src="../assets/img/reflux_bad.gif" height="250" width="400"  alt="A typical embolization catheter can lead to off-target embolization."/>
                <p style="text-align:center;">Standard embolization can result in reflux.</p>
            </div>
                <div style="float:left;margin-right:5px;">
                <img class="middle-img" src="../assets/img/emboquant_system.gif" height="250" width="400" alt="Our pressure sensing system prevents off-target embolization." />
                <p style="text-align:center;">Our system optimizes embolization level.</p>
            </div>
        </div>

            <p style="clear: both;">
            <strong> Tracking Surgical Instruments in the Operating Room </strong><ul>
            <li> Rationale: <ul>
                <li> Surgeries are expensive for hospitals, and one of the primary drivers for this is costly operational inefficiencies due to poor instrument management. </li>
                <li> Each instrument that is taken out for a given procedure needs to be re-sterilized. While this is only around $0.50 per instrument, the cost adds up quickly when we consider potentially a hundred instruments per surgery and hundreds of surgeries per day in a hospital.</li>
                <li> One study demonstrated that around 70% of instruments could be safely removed without any added risk to the patient. </li>
                <li> Additionally, many instruments end up lost, or even left inside the patient after surgery, leading to high cost of replacement or litigation costs.</li>
                <li> Perioperative administrators would love to be able to reduce these risks and inefficiencies, but they currently lack instrument-level insight into what's going on in the operating room.</li>
                <li> Computer vision could potentially be used to track instruments and determine which are actually needed in a given procedure. </li>
                <li> With a conservative 40% reduction in instruments used, this would amount to an estimated $1.2 million dollar saving per year at Johns Hopkins Hospital alone.</li>
            </ul></li>
            <li> Project:<ul>
                <li> We built a CNN-based computer vision system to track surgical instruments in the operating room. </li>
                <li> We collaborated with and interviewed many perioperative administrators, clinicians, nurses, and scrub techs to identify practical and regulatory requirements for our system.</li>
                </ul>
            </li>
            <li>Outcomes:
                <ul class="nav nav-pills nav-stacked">
                <li> We trained a CNN that could accurately identify the surgical instrument being held in a video frame.</li>
                <li> We used optical flow to reduce data requirements by over 90%, and implemented temporal post-processing to fix frame-level mistakes, as well as logic/rule-based event processing to produce instrument usage requirements to administrators.</li>
                <li> We created a small annotated video dataset of instruments being moved over a fake operating room instrument table.</li>
            </ul>
            </li>
        </ul>
            <div class="img-parent-box" style="text-align:center">
                <div style="float:left;margin-right:5px;">
                <img src="../assets/img/monitor.gif" height="338" width="600"  alt="Our system tracks the instrument being held."/>
                <p style="text-align:center;">Our system tracks the instrument being held.</p>
            </div> </div>

            <p style="clear: both;">
            <strong> Epilepsy localization </strong><ul>
            <li> Rationale: <ul>
                <li> Over sixty million people worldwide suffer from epilepsy. For about one third of these people, their seizures are focal-onset, meaning they begin in particular regions of the brain, and for about half of those people, their seizures are medically-refractory, meaning they don't adequately respond to medical treatments.</li>
                <li> For these about 10 mil. medically refractory focal-onset patients, the standard treatment is to surgically resect (or potentially neurostimulate) the culprit "epileptogenic zone". </li>
                <li> However, this resection has an abysmal success rate of about 50%, and one potential theory for why is that surgeons are not targeting the correct region for resection.</li>
                <li> Thus, better localization of the epileptogenic zone could help improve surgical outcomes. </li>
                <li> Normally, this localization is done by implanting invasive electrodes directly on the brain (ECoG or iEEG), or through the brain (sEEG). The electrical activity is monitored, sometimes for days, to identify problematic regions.</li>
                <li> Interictal spikes, or spikes occuring in between seizures are one promising feature that can be seen on these EEG recordings to identify the epileptogenic zone. </li>
                <li> Better placement of the invasive electrodes themselves could also improve a clinician's ability localize the epileptogenic zone.</li>
            </ul></li>
            <li> Project:<ul>
                <li> We built a simple signal-processing + threshold based algorithm to identify interictal spikes in iEEG. </li>
                <li> We conducted research to try to see if the recordings from non-invasive EEG could be used to identify optimal invasive electrode placement. </li>
                </ul>
            </li>
            <li>Outcomes:
                <ul class="nav nav-pills nav-stacked">
                <li> We were able to quickly and accurately process a lot of iEEG data to identify presence and location of interictal spikes.</li>
                <li> We determined that we could isolate the power of the signal in the gamma band for non-invasive scalp EEG. Furthermore, we showed that when the distribution of this power over the scalp regions was more similar to the distribution of physical invasive electrodes, resection was more likely to be successful.</li>
            </ul>
            </li>
        </ul> <br> </p>


        </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'] ],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
